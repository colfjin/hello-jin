{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 폐암 환자 데이터셋을 활용한 이진 분류\n",
    "\n",
    "- 17개의 특징으로 구성\n",
    "- 라벨은 생존(1),사망(0) 클래스로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.19</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.06</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1     2     3   4   5   6   7   8   9   10  11  12  13  14  15  16  17\n",
       "0  293   1  3.80  2.80   0   0   0   0   0   0  12   0   0   0   1   0  62   0\n",
       "1    1   2  2.88  2.16   1   0   0   0   1   1  14   0   0   0   1   0  60   0\n",
       "2    8   2  3.19  2.50   1   0   0   0   1   0  11   0   0   1   1   0  66   1\n",
       "3   14   2  3.98  3.06   2   0   0   0   1   1  14   0   0   0   1   0  80   1\n",
       "4   17   2  2.21  1.88   0   0   1   0   0   0  12   0   0   0   1   0  56   0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"./data/ThoraricSurgery.csv\",header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 470 entries, 0 to 469\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       470 non-null    int64  \n",
      " 1   1       470 non-null    int64  \n",
      " 2   2       470 non-null    float64\n",
      " 3   3       470 non-null    float64\n",
      " 4   4       470 non-null    int64  \n",
      " 5   5       470 non-null    int64  \n",
      " 6   6       470 non-null    int64  \n",
      " 7   7       470 non-null    int64  \n",
      " 8   8       470 non-null    int64  \n",
      " 9   9       470 non-null    int64  \n",
      " 10  10      470 non-null    int64  \n",
      " 11  11      470 non-null    int64  \n",
      " 12  12      470 non-null    int64  \n",
      " 13  13      470 non-null    int64  \n",
      " 14  14      470 non-null    int64  \n",
      " 15  15      470 non-null    int64  \n",
      " 16  16      470 non-null    int64  \n",
      " 17  17      470 non-null    int64  \n",
      "dtypes: float64(2), int64(16)\n",
      "memory usage: 66.2 KB\n"
     ]
    }
   ],
   "source": [
    "#결측치\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>235.500000</td>\n",
       "      <td>3.095745</td>\n",
       "      <td>3.281638</td>\n",
       "      <td>4.568702</td>\n",
       "      <td>0.780851</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>0.144681</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>0.687234</td>\n",
       "      <td>0.165957</td>\n",
       "      <td>11.736170</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>62.534043</td>\n",
       "      <td>0.148936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>135.821574</td>\n",
       "      <td>0.722309</td>\n",
       "      <td>0.871395</td>\n",
       "      <td>11.767857</td>\n",
       "      <td>0.535375</td>\n",
       "      <td>0.248472</td>\n",
       "      <td>0.352154</td>\n",
       "      <td>0.248472</td>\n",
       "      <td>0.464114</td>\n",
       "      <td>0.372439</td>\n",
       "      <td>0.702243</td>\n",
       "      <td>0.262811</td>\n",
       "      <td>0.065163</td>\n",
       "      <td>0.129488</td>\n",
       "      <td>0.383529</td>\n",
       "      <td>0.065163</td>\n",
       "      <td>8.706902</td>\n",
       "      <td>0.356405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>118.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>235.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>352.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.807500</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>470.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>86.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  470.000000  470.000000  470.000000  470.000000  470.000000  470.000000   \n",
       "mean   235.500000    3.095745    3.281638    4.568702    0.780851    0.065957   \n",
       "std    135.821574    0.722309    0.871395   11.767857    0.535375    0.248472   \n",
       "min      1.000000    1.000000    1.440000    0.960000    0.000000    0.000000   \n",
       "25%    118.250000    3.000000    2.600000    1.960000    0.000000    0.000000   \n",
       "50%    235.500000    3.000000    3.160000    2.400000    1.000000    0.000000   \n",
       "75%    352.750000    3.000000    3.807500    3.080000    1.000000    0.000000   \n",
       "max    470.000000    8.000000    6.300000   86.300000    2.000000    1.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  470.000000  470.000000  470.000000  470.000000  470.000000  470.000000   \n",
       "mean     0.144681    0.065957    0.687234    0.165957   11.736170    0.074468   \n",
       "std      0.352154    0.248472    0.464114    0.372439    0.702243    0.262811   \n",
       "min      0.000000    0.000000    0.000000    0.000000   11.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000   11.000000    0.000000   \n",
       "50%      0.000000    0.000000    1.000000    0.000000   12.000000    0.000000   \n",
       "75%      0.000000    0.000000    1.000000    0.000000   12.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000   14.000000    1.000000   \n",
       "\n",
       "               12          13          14          15          16          17  \n",
       "count  470.000000  470.000000  470.000000  470.000000  470.000000  470.000000  \n",
       "mean     0.004255    0.017021    0.821277    0.004255   62.534043    0.148936  \n",
       "std      0.065163    0.129488    0.383529    0.065163    8.706902    0.356405  \n",
       "min      0.000000    0.000000    0.000000    0.000000   21.000000    0.000000  \n",
       "25%      0.000000    0.000000    1.000000    0.000000   57.000000    0.000000  \n",
       "50%      0.000000    0.000000    1.000000    0.000000   62.000000    0.000000  \n",
       "75%      0.000000    0.000000    1.000000    0.000000   69.000000    0.000000  \n",
       "max      1.000000    1.000000    1.000000    1.000000   87.000000    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이상치\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007878</td>\n",
       "      <td>-0.034985</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>-0.021449</td>\n",
       "      <td>0.052724</td>\n",
       "      <td>0.027238</td>\n",
       "      <td>-0.061885</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>0.027271</td>\n",
       "      <td>0.047079</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>-0.016382</td>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.038681</td>\n",
       "      <td>-0.042400</td>\n",
       "      <td>-0.005826</td>\n",
       "      <td>-0.074924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.121263</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>-0.055900</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>-0.062958</td>\n",
       "      <td>-0.047142</td>\n",
       "      <td>-0.075850</td>\n",
       "      <td>-0.019562</td>\n",
       "      <td>-0.135050</td>\n",
       "      <td>0.029753</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>-0.017461</td>\n",
       "      <td>-0.107427</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.060444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.034985</td>\n",
       "      <td>0.121263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032975</td>\n",
       "      <td>-0.091094</td>\n",
       "      <td>0.019786</td>\n",
       "      <td>-0.095827</td>\n",
       "      <td>0.055829</td>\n",
       "      <td>-0.052770</td>\n",
       "      <td>-0.100242</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>-0.115145</td>\n",
       "      <td>-0.009135</td>\n",
       "      <td>-0.035584</td>\n",
       "      <td>-0.012009</td>\n",
       "      <td>-0.060578</td>\n",
       "      <td>-0.290178</td>\n",
       "      <td>-0.046374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>0.032975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.143155</td>\n",
       "      <td>0.161615</td>\n",
       "      <td>0.102979</td>\n",
       "      <td>0.260073</td>\n",
       "      <td>-0.099914</td>\n",
       "      <td>-0.086103</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>-0.022251</td>\n",
       "      <td>-0.013617</td>\n",
       "      <td>-0.025088</td>\n",
       "      <td>-0.100853</td>\n",
       "      <td>-0.016509</td>\n",
       "      <td>-0.115900</td>\n",
       "      <td>-0.042841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.021449</td>\n",
       "      <td>-0.055900</td>\n",
       "      <td>-0.091094</td>\n",
       "      <td>-0.143155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092863</td>\n",
       "      <td>0.123296</td>\n",
       "      <td>0.092863</td>\n",
       "      <td>0.684647</td>\n",
       "      <td>0.418042</td>\n",
       "      <td>0.089751</td>\n",
       "      <td>0.025310</td>\n",
       "      <td>0.026788</td>\n",
       "      <td>0.023166</td>\n",
       "      <td>0.172289</td>\n",
       "      <td>-0.034330</td>\n",
       "      <td>0.214528</td>\n",
       "      <td>0.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052724</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.019786</td>\n",
       "      <td>0.161615</td>\n",
       "      <td>0.092863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.256225</td>\n",
       "      <td>0.067529</td>\n",
       "      <td>-0.024115</td>\n",
       "      <td>-0.072455</td>\n",
       "      <td>0.099942</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>-0.034968</td>\n",
       "      <td>-0.077406</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>0.044789</td>\n",
       "      <td>0.057375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.027238</td>\n",
       "      <td>-0.062958</td>\n",
       "      <td>-0.095827</td>\n",
       "      <td>0.102979</td>\n",
       "      <td>0.123296</td>\n",
       "      <td>0.256225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134386</td>\n",
       "      <td>0.081772</td>\n",
       "      <td>0.060393</td>\n",
       "      <td>0.059840</td>\n",
       "      <td>-0.001471</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>0.086156</td>\n",
       "      <td>-0.044942</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>0.086705</td>\n",
       "      <td>0.065785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.061885</td>\n",
       "      <td>-0.047142</td>\n",
       "      <td>0.055829</td>\n",
       "      <td>0.260073</td>\n",
       "      <td>0.092863</td>\n",
       "      <td>0.067529</td>\n",
       "      <td>0.134386</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049843</td>\n",
       "      <td>-0.072455</td>\n",
       "      <td>0.075502</td>\n",
       "      <td>-0.042725</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>0.097572</td>\n",
       "      <td>-0.077406</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>-0.015331</td>\n",
       "      <td>0.105530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008845</td>\n",
       "      <td>-0.075850</td>\n",
       "      <td>-0.052770</td>\n",
       "      <td>-0.099914</td>\n",
       "      <td>0.684647</td>\n",
       "      <td>-0.024115</td>\n",
       "      <td>0.081772</td>\n",
       "      <td>0.049843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202245</td>\n",
       "      <td>0.145345</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>0.044101</td>\n",
       "      <td>0.017815</td>\n",
       "      <td>0.200373</td>\n",
       "      <td>-0.026401</td>\n",
       "      <td>0.149589</td>\n",
       "      <td>0.088860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.027271</td>\n",
       "      <td>-0.019562</td>\n",
       "      <td>-0.100242</td>\n",
       "      <td>-0.086103</td>\n",
       "      <td>0.418042</td>\n",
       "      <td>-0.072455</td>\n",
       "      <td>0.060393</td>\n",
       "      <td>-0.072455</td>\n",
       "      <td>0.202245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.036044</td>\n",
       "      <td>0.069522</td>\n",
       "      <td>0.058695</td>\n",
       "      <td>0.029726</td>\n",
       "      <td>0.118527</td>\n",
       "      <td>-0.029161</td>\n",
       "      <td>0.208003</td>\n",
       "      <td>0.086467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.047079</td>\n",
       "      <td>-0.135050</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.089751</td>\n",
       "      <td>0.099942</td>\n",
       "      <td>0.059840</td>\n",
       "      <td>0.075502</td>\n",
       "      <td>0.145345</td>\n",
       "      <td>-0.036044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037363</td>\n",
       "      <td>-0.022009</td>\n",
       "      <td>-0.020854</td>\n",
       "      <td>0.038303</td>\n",
       "      <td>-0.022009</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.174371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.009229</td>\n",
       "      <td>0.029753</td>\n",
       "      <td>-0.115145</td>\n",
       "      <td>-0.022251</td>\n",
       "      <td>0.025310</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>-0.001471</td>\n",
       "      <td>-0.042725</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>0.069522</td>\n",
       "      <td>0.037363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018543</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>-0.036906</td>\n",
       "      <td>-0.018543</td>\n",
       "      <td>0.085081</td>\n",
       "      <td>0.108974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.016382</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>-0.009135</td>\n",
       "      <td>-0.013617</td>\n",
       "      <td>0.026788</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>0.044101</td>\n",
       "      <td>0.058695</td>\n",
       "      <td>-0.022009</td>\n",
       "      <td>-0.018543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008602</td>\n",
       "      <td>0.030496</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.030320</td>\n",
       "      <td>-0.027347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.017461</td>\n",
       "      <td>-0.035584</td>\n",
       "      <td>-0.025088</td>\n",
       "      <td>0.023166</td>\n",
       "      <td>-0.034968</td>\n",
       "      <td>0.086156</td>\n",
       "      <td>0.097572</td>\n",
       "      <td>0.017815</td>\n",
       "      <td>0.029726</td>\n",
       "      <td>-0.020854</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>-0.008602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>-0.008602</td>\n",
       "      <td>0.058112</td>\n",
       "      <td>0.037354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.038681</td>\n",
       "      <td>-0.107427</td>\n",
       "      <td>-0.012009</td>\n",
       "      <td>-0.100853</td>\n",
       "      <td>0.172289</td>\n",
       "      <td>-0.077406</td>\n",
       "      <td>-0.044942</td>\n",
       "      <td>-0.077406</td>\n",
       "      <td>0.200373</td>\n",
       "      <td>0.118527</td>\n",
       "      <td>0.038303</td>\n",
       "      <td>-0.036906</td>\n",
       "      <td>0.030496</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.054820</td>\n",
       "      <td>0.068869</td>\n",
       "      <td>0.085958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.042400</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>-0.060578</td>\n",
       "      <td>-0.016509</td>\n",
       "      <td>-0.034330</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>-0.026401</td>\n",
       "      <td>-0.029161</td>\n",
       "      <td>-0.022009</td>\n",
       "      <td>-0.018543</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.008602</td>\n",
       "      <td>-0.054820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019046</td>\n",
       "      <td>-0.027347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.005826</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>-0.290178</td>\n",
       "      <td>-0.115900</td>\n",
       "      <td>0.214528</td>\n",
       "      <td>0.044789</td>\n",
       "      <td>0.086705</td>\n",
       "      <td>-0.015331</td>\n",
       "      <td>0.149589</td>\n",
       "      <td>0.208003</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.085081</td>\n",
       "      <td>-0.030320</td>\n",
       "      <td>0.058112</td>\n",
       "      <td>0.068869</td>\n",
       "      <td>-0.019046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.074924</td>\n",
       "      <td>0.060444</td>\n",
       "      <td>-0.046374</td>\n",
       "      <td>-0.042841</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>0.057375</td>\n",
       "      <td>0.065785</td>\n",
       "      <td>0.105530</td>\n",
       "      <td>0.088860</td>\n",
       "      <td>0.086467</td>\n",
       "      <td>0.174371</td>\n",
       "      <td>0.108974</td>\n",
       "      <td>-0.027347</td>\n",
       "      <td>0.037354</td>\n",
       "      <td>0.085958</td>\n",
       "      <td>-0.027347</td>\n",
       "      <td>0.038902</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000 -0.007878 -0.034985  0.029563 -0.021449  0.052724  0.027238   \n",
       "1  -0.007878  1.000000  0.121263  0.058780 -0.055900  0.047900 -0.062958   \n",
       "2  -0.034985  0.121263  1.000000  0.032975 -0.091094  0.019786 -0.095827   \n",
       "3   0.029563  0.058780  0.032975  1.000000 -0.143155  0.161615  0.102979   \n",
       "4  -0.021449 -0.055900 -0.091094 -0.143155  1.000000  0.092863  0.123296   \n",
       "5   0.052724  0.047900  0.019786  0.161615  0.092863  1.000000  0.256225   \n",
       "6   0.027238 -0.062958 -0.095827  0.102979  0.123296  0.256225  1.000000   \n",
       "7  -0.061885 -0.047142  0.055829  0.260073  0.092863  0.067529  0.134386   \n",
       "8   0.008845 -0.075850 -0.052770 -0.099914  0.684647 -0.024115  0.081772   \n",
       "9   0.027271 -0.019562 -0.100242 -0.086103  0.418042 -0.072455  0.060393   \n",
       "10  0.047079 -0.135050  0.034088  0.015504  0.089751  0.099942  0.059840   \n",
       "11 -0.009229  0.029753 -0.115145 -0.022251  0.025310  0.022578 -0.001471   \n",
       "12 -0.016382 -0.008675 -0.009135 -0.013617  0.026788 -0.017372 -0.026886   \n",
       "13 -0.015761 -0.017461 -0.035584 -0.025088  0.023166 -0.034968  0.086156   \n",
       "14 -0.038681 -0.107427 -0.012009 -0.100853  0.172289 -0.077406 -0.044942   \n",
       "15 -0.042400 -0.008675 -0.060578 -0.016509 -0.034330 -0.017372 -0.026886   \n",
       "16 -0.005826  0.076271 -0.290178 -0.115900  0.214528  0.044789  0.086705   \n",
       "17 -0.074924  0.060444 -0.046374 -0.042841  0.093200  0.057375  0.065785   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0  -0.061885  0.008845  0.027271  0.047079 -0.009229 -0.016382 -0.015761   \n",
       "1  -0.047142 -0.075850 -0.019562 -0.135050  0.029753 -0.008675 -0.017461   \n",
       "2   0.055829 -0.052770 -0.100242  0.034088 -0.115145 -0.009135 -0.035584   \n",
       "3   0.260073 -0.099914 -0.086103  0.015504 -0.022251 -0.013617 -0.025088   \n",
       "4   0.092863  0.684647  0.418042  0.089751  0.025310  0.026788  0.023166   \n",
       "5   0.067529 -0.024115 -0.072455  0.099942  0.022578 -0.017372 -0.034968   \n",
       "6   0.134386  0.081772  0.060393  0.059840 -0.001471 -0.026886  0.086156   \n",
       "7   1.000000  0.049843 -0.072455  0.075502 -0.042725 -0.017372  0.097572   \n",
       "8   0.049843  1.000000  0.202245  0.145345  0.016551  0.044101  0.017815   \n",
       "9  -0.072455  0.202245  1.000000 -0.036044  0.069522  0.058695  0.029726   \n",
       "10  0.075502  0.145345 -0.036044  1.000000  0.037363 -0.022009 -0.020854   \n",
       "11 -0.042725  0.016551  0.069522  0.037363  1.000000 -0.018543  0.025328   \n",
       "12 -0.017372  0.044101  0.058695 -0.022009 -0.018543  1.000000 -0.008602   \n",
       "13  0.097572  0.017815  0.029726 -0.020854  0.025328 -0.008602  1.000000   \n",
       "14 -0.077406  0.200373  0.118527  0.038303 -0.036906  0.030496  0.061386   \n",
       "15 -0.017372 -0.026401 -0.029161 -0.022009 -0.018543 -0.004274 -0.008602   \n",
       "16 -0.015331  0.149589  0.208003  0.016118  0.085081 -0.030320  0.058112   \n",
       "17  0.105530  0.088860  0.086467  0.174371  0.108974 -0.027347  0.037354   \n",
       "\n",
       "          14        15        16        17  \n",
       "0  -0.038681 -0.042400 -0.005826 -0.074924  \n",
       "1  -0.107427 -0.008675  0.076271  0.060444  \n",
       "2  -0.012009 -0.060578 -0.290178 -0.046374  \n",
       "3  -0.100853 -0.016509 -0.115900 -0.042841  \n",
       "4   0.172289 -0.034330  0.214528  0.093200  \n",
       "5  -0.077406 -0.017372  0.044789  0.057375  \n",
       "6  -0.044942 -0.026886  0.086705  0.065785  \n",
       "7  -0.077406 -0.017372 -0.015331  0.105530  \n",
       "8   0.200373 -0.026401  0.149589  0.088860  \n",
       "9   0.118527 -0.029161  0.208003  0.086467  \n",
       "10  0.038303 -0.022009  0.016118  0.174371  \n",
       "11 -0.036906 -0.018543  0.085081  0.108974  \n",
       "12  0.030496 -0.004274 -0.030320 -0.027347  \n",
       "13  0.061386 -0.008602  0.058112  0.037354  \n",
       "14  1.000000 -0.054820  0.068869  0.085958  \n",
       "15 -0.054820  1.000000 -0.019046 -0.027347  \n",
       "16  0.068869 -0.019046  1.000000  0.038902  \n",
       "17  0.085958 -0.027347  0.038902  1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#상관관계\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((470, 17), (470,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X,y로 분리\n",
    "X=data.iloc[:,:-1]\n",
    "y=data.iloc[:,-1]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w,b를 초기화하기 위한 seed를 설정\n",
    "seed=7\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망 설계\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 34)                612       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 35        \n",
      "=================================================================\n",
      "Total params: 647\n",
      "Trainable params: 647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#새로운 신경망 생성\n",
    "model=Sequential()\n",
    "#신경망에 입력층을 추가\n",
    "#Dense 첫 파라미터는 가급적 2의 n승을 맞춰서 해줌\n",
    "model.add(Dense(34,input_dim=17,activation=\"relu\"))\n",
    "#신경망에 출력층을 추가\n",
    "#회귀 : linear(기본값이라 안써도 됨)\n",
    "#이진분류 : 1(sigmoid),2(softmax,y를 원핫인코딩한 경우)\n",
    "#다중분류 : softmax\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#컴파일\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "47/47 [==============================] - 0s 520us/step - loss: 7.8274 - acc: 0.7680\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 1.4967 - acc: 0.7098\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.5225 - acc: 0.8515\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4477 - acc: 0.8657\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4878 - acc: 0.8531\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4375 - acc: 0.8556\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.5377 - acc: 0.7890\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.5734 - acc: 0.7809\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.4932 - acc: 0.8513\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4204 - acc: 0.8553\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4439 - acc: 0.8533\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 694us/step - loss: 0.3913 - acc: 0.8723\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4041 - acc: 0.8791\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4965 - acc: 0.8123\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.5723 - acc: 0.7826\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4179 - acc: 0.8533\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4775 - acc: 0.8059\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4673 - acc: 0.8189\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 549us/step - loss: 0.4765 - acc: 0.8431\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4583 - acc: 0.8230\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 412us/step - loss: 0.5250 - acc: 0.8038\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.3915 - acc: 0.8575\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4173 - acc: 0.8561\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 412us/step - loss: 0.4498 - acc: 0.8391\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.3751 - acc: 0.8606\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4759 - acc: 0.8294\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.4851 - acc: 0.8236\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4235 - acc: 0.8510\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.4391 - acc: 0.8583\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.5065 - acc: 0.7705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18aab97c970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습\n",
    "model.fit(X,y,epochs=30,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 500us/step - loss: 0.3895 - acc: 0.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3894892930984497, 0.8510638475418091]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 34)                612       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17)                595       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 18        \n",
      "=================================================================\n",
      "Total params: 1,225\n",
      "Trainable params: 1,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "\n",
    "#입력층\n",
    "model1.add(Dense(34,input_dim=17,activation=\"relu\"))\n",
    "\n",
    "#은닉층 추가\n",
    "model1.add(Dense(17,activation=\"relu\"))\n",
    "\n",
    "#출력층\n",
    "model1.add(Dense(1,activation=\"sigmoid\"))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "47/47 [==============================] - 0s 521us/step - loss: 2.2515 - acc: 0.7848\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 521us/step - loss: 0.5954 - acc: 0.8218\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4894 - acc: 0.8595\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 586us/step - loss: 0.4943 - acc: 0.8479\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 498us/step - loss: 0.4742 - acc: 0.8427\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4254 - acc: 0.8532\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.6496 - acc: 0.7685\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.5742 - acc: 0.7669\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4931 - acc: 0.8568\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4159 - acc: 0.8561\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4938 - acc: 0.8441\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.3859 - acc: 0.8694\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4299 - acc: 0.8811\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.5524 - acc: 0.7868\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.5454 - acc: 0.8319\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4276 - acc: 0.8531\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4936 - acc: 0.8223\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.5160 - acc: 0.8146\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.5047 - acc: 0.8301\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.5262 - acc: 0.8206\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.5188 - acc: 0.7985\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4107 - acc: 0.8661\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4232 - acc: 0.8525\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 434us/step - loss: 0.4983 - acc: 0.8163\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.3871 - acc: 0.8553\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4464 - acc: 0.8402\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.4256 - acc: 0.8448\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4170 - acc: 0.8527\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4133 - acc: 0.8588\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.5093 - acc: 0.7848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18aacf55400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X,y,epochs=30,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 498us/step - loss: 0.3895 - acc: 0.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38945600390434265, 0.8510638475418091]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               2304      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 35,585\n",
      "Trainable params: 35,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2=Sequential()\n",
    "\n",
    "#입력층\n",
    "model2.add(Dense(128,input_dim=17,activation=\"relu\"))\n",
    "\n",
    "#은닉층 추가\n",
    "model2.add(Dense(256,activation=\"relu\"))\n",
    "\n",
    "#출력층\n",
    "model2.add(Dense(1,activation=\"sigmoid\"))\n",
    "model2.summary()\n",
    "model2.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "47/47 [==============================] - 0s 607us/step - loss: 3.5890 - acc: 0.7395\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 498us/step - loss: 0.6812 - acc: 0.8013\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.7147 - acc: 0.7835\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.6052 - acc: 0.8009\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 672us/step - loss: 0.5083 - acc: 0.8487\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.5245 - acc: 0.8405\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.6822 - acc: 0.7796\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.6179 - acc: 0.7696\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.7666 - acc: 0.7734\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.5186 - acc: 0.8413\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.5430 - acc: 0.8428\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.7466 - acc: 0.8257\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.5400 - acc: 0.8555\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.5119 - acc: 0.8489\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.7027 - acc: 0.7844\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.5271 - acc: 0.8391\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.5238 - acc: 0.8388\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.6566 - acc: 0.7734\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.5496 - acc: 0.8138\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.7638 - acc: 0.7717\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.5471 - acc: 0.8042\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 561us/step - loss: 0.5475 - acc: 0.8268\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.5337 - acc: 0.8227\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4740 - acc: 0.8127\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4077 - acc: 0.8590\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.5212 - acc: 0.8242\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.4412 - acc: 0.8410\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4383 - acc: 0.8568\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 522us/step - loss: 0.5823 - acc: 0.7907\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 672us/step - loss: 0.5887 - acc: 0.7002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18aae25d4c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X,y,epochs=30,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_109 (Dense)            (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 34)                612       \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 68)                2380      \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 34)                2346      \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 17)                595       \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 1)                 18        \n",
      "=================================================================\n",
      "Total params: 6,257\n",
      "Trainable params: 6,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3=Sequential()\n",
    "\n",
    "#입력층\n",
    "model3.add(Dense(17,input_dim=17,activation=\"relu\"))\n",
    "#은닉층 추가\n",
    "model3.add(Dense(34,activation=\"relu\"))\n",
    "model3.add(Dense(68,activation=\"relu\"))\n",
    "model3.add(Dense(34,activation=\"relu\"))\n",
    "model3.add(Dense(17,activation=\"relu\"))\n",
    "#출력층\n",
    "model3.add(Dense(1,activation=\"sigmoid\"))\n",
    "model3.summary()\n",
    "model3.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 3ms/step - loss: 2.5725 - acc: 0.3336\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.7870 - acc: 0.8398\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4467 - acc: 0.8534\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4494 - acc: 0.8552\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4562 - acc: 0.8540\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4392 - acc: 0.8540\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4590 - acc: 0.8452\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6818 - acc: 0.8518\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.4814 - acc: 0.8510\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4177 - acc: 0.8566\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4564 - acc: 0.8585\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.8171 - acc: 0.5100\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6649 - acc: 0.8543\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4270 - acc: 0.8532\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4413 - acc: 0.8566\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5887 - acc: 0.8543\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4454 - acc: 0.8488\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.5400 - acc: 0.8521\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4830 - acc: 0.8521\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4900 - acc: 0.8507\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.5657 - acc: 0.8407\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.7925 - acc: 0.8529\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 996us/step - loss: 0.4273 - acc: 0.8543\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4581 - acc: 0.8488\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.5416 - acc: 0.8532\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4882 - acc: 0.8507\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4308 - acc: 0.8499\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4947 - acc: 0.8507\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5018 - acc: 0.8499\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6898 - acc: 0.8543\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4195 - acc: 0.8496\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4034 - acc: 0.8588\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5248 - acc: 0.8588\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4120 - acc: 0.8577\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4353 - acc: 0.8466\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4141 - acc: 0.8521\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4181 - acc: 0.8532\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4368 - acc: 0.8588\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5177 - acc: 0.8452\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6281 - acc: 0.8518\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4500 - acc: 0.8510\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4211 - acc: 0.8488\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4936 - acc: 0.8510\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5586 - acc: 0.8485\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.4238 - acc: 0.8463\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4024 - acc: 0.8577\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4609 - acc: 0.8488\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4664 - acc: 0.8599\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4970 - acc: 0.8418\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4742 - acc: 0.8532\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.5033 - acc: 0.8485\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4370 - acc: 0.8555\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4120 - acc: 0.8521\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3984 - acc: 0.8599\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4373 - acc: 0.8455\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4186 - acc: 0.8532\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5324 - acc: 0.8532\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.4889 - acc: 0.8499\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.5481 - acc: 0.8563\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4785 - acc: 0.8477\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4207 - acc: 0.8499\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4212 - acc: 0.8566\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4255 - acc: 0.8532\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4509 - acc: 0.8566\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4698 - acc: 0.8510\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4618 - acc: 0.8532\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4232 - acc: 0.8488\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.5166 - acc: 0.8499\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4468 - acc: 0.8643\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5759 - acc: 0.8532\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4275 - acc: 0.8610\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4457 - acc: 0.8532\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4122 - acc: 0.8532\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4185 - acc: 0.8599\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4531 - acc: 0.8455\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4218 - acc: 0.8521\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4287 - acc: 0.8577\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4520 - acc: 0.8555\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4392 - acc: 0.8466\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4203 - acc: 0.8499\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4374 - acc: 0.8543\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4205 - acc: 0.8621\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.5195 - acc: 0.8488\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4303 - acc: 0.8543\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4365 - acc: 0.8521\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4279 - acc: 0.8555\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4160 - acc: 0.8477\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4383 - acc: 0.8477\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.5787 - acc: 0.8418\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4411 - acc: 0.8521\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4158 - acc: 0.8555\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4018 - acc: 0.8555\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4231 - acc: 0.8555\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5771 - acc: 0.8555\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4635 - acc: 0.8499\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4216 - acc: 0.8543\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4235 - acc: 0.8510\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4010 - acc: 0.8588\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4265 - acc: 0.8555\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4103 - acc: 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18abf7c8820>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X,y,epochs=100,batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 570us/step - loss: 0.4044 - acc: 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4043582081794739, 0.8531914949417114]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제값 : 0\t 예측값 [0.08831817]\n",
      "실제값 : 0\t 예측값 [0.10814926]\n",
      "실제값 : 1\t 예측값 [0.48992684]\n",
      "실제값 : 1\t 예측값 [0.33813292]\n",
      "실제값 : 0\t 예측값 [0.10479727]\n",
      "실제값 : 0\t 예측값 [0.09001258]\n",
      "실제값 : 0\t 예측값 [0.1697509]\n",
      "실제값 : 1\t 예측값 [0.20049879]\n",
      "실제값 : 0\t 예측값 [0.19138545]\n",
      "실제값 : 0\t 예측값 [0.14549899]\n"
     ]
    }
   ],
   "source": [
    "#실제값하고 예측값을 비교\n",
    "pred=model1.predict(X)\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"실제값 : {}\\t 예측값 {}\".format(y[i], pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이진분류를 y를 원핫인코딩해서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_en=pd.get_dummies(y)\n",
    "y_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0    1  0\n",
       "1    1  0\n",
       "2    0  1\n",
       "3    0  1\n",
       "4    1  0\n",
       "..  .. ..\n",
       "465  1  0\n",
       "466  1  0\n",
       "467  1  0\n",
       "468  0  1\n",
       "469  1  0\n",
       "\n",
       "[470 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_133 (Dense)            (None, 34)                612       \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 51)                1785      \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 68)                3536      \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 34)                2346      \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 17)                595       \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 2)                 36        \n",
      "=================================================================\n",
      "Total params: 8,910\n",
      "Trainable params: 8,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4=Sequential()\n",
    "\n",
    "#입력층\n",
    "model4.add(Dense(34,input_dim=17,activation=\"relu\"))\n",
    "#은닉층 추가\n",
    "model4.add(Dense(51,activation=\"relu\"))\n",
    "model4.add(Dense(68,activation=\"relu\"))\n",
    "model4.add(Dense(34,activation=\"relu\"))\n",
    "model4.add(Dense(17,activation=\"relu\"))\n",
    "#출력층\n",
    "model4.add(Dense(2,activation=\"softmax\"))\n",
    "model4.summary()\n",
    "model4.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/47 [==============================] - 1s 564us/step - loss: 0.8248 - acc: 0.8280\n",
      "Epoch 2/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4510 - acc: 0.8560\n",
      "Epoch 3/100\n",
      "47/47 [==============================] - 0s 651us/step - loss: 0.3962 - acc: 0.8704\n",
      "Epoch 4/100\n",
      "47/47 [==============================] - 0s 628us/step - loss: 0.4139 - acc: 0.8682\n",
      "Epoch 5/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4264 - acc: 0.8565\n",
      "Epoch 6/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4166 - acc: 0.8560\n",
      "Epoch 7/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4965 - acc: 0.8289\n",
      "Epoch 8/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4651 - acc: 0.8368\n",
      "Epoch 9/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4348 - acc: 0.8499\n",
      "Epoch 10/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.3958 - acc: 0.8674\n",
      "Epoch 11/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.4302 - acc: 0.8551\n",
      "Epoch 12/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.3993 - acc: 0.8723\n",
      "Epoch 13/100\n",
      "47/47 [==============================] - 0s 488us/step - loss: 0.3811 - acc: 0.8866\n",
      "Epoch 14/100\n",
      "47/47 [==============================] - 0s 455us/step - loss: 0.4485 - acc: 0.8565\n",
      "Epoch 15/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4590 - acc: 0.8328\n",
      "Epoch 16/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4175 - acc: 0.8562\n",
      "Epoch 17/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4352 - acc: 0.8413\n",
      "Epoch 18/100\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4927 - acc: 0.8195\n",
      "Epoch 19/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4351 - acc: 0.8455\n",
      "Epoch 20/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4678 - acc: 0.8227\n",
      "Epoch 21/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4855 - acc: 0.8110\n",
      "Epoch 22/100\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.3871 - acc: 0.8691\n",
      "Epoch 23/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4131 - acc: 0.8567\n",
      "Epoch 24/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4467 - acc: 0.8336\n",
      "Epoch 25/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.3906 - acc: 0.8653\n",
      "Epoch 26/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4429 - acc: 0.8361\n",
      "Epoch 27/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4425 - acc: 0.8450\n",
      "Epoch 28/100\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4288 - acc: 0.8492\n",
      "Epoch 29/100\n",
      "47/47 [==============================] - 0s 477us/step - loss: 0.3999 - acc: 0.8613\n",
      "Epoch 30/100\n",
      "47/47 [==============================] - 0s 672us/step - loss: 0.4707 - acc: 0.8322\n",
      "Epoch 31/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.3803 - acc: 0.8697\n",
      "Epoch 32/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4094 - acc: 0.8519\n",
      "Epoch 33/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.3933 - acc: 0.8648\n",
      "Epoch 34/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4122 - acc: 0.8706\n",
      "Epoch 35/100\n",
      "47/47 [==============================] - 0s 672us/step - loss: 0.4771 - acc: 0.8142\n",
      "Epoch 36/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4160 - acc: 0.8605\n",
      "Epoch 37/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.4151 - acc: 0.8496\n",
      "Epoch 38/100\n",
      "47/47 [==============================] - 0s 715us/step - loss: 0.3706 - acc: 0.8779\n",
      "Epoch 39/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4572 - acc: 0.8223\n",
      "Epoch 40/100\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.4312 - acc: 0.8479\n",
      "Epoch 41/100\n",
      "47/47 [==============================] - 0s 608us/step - loss: 0.4295 - acc: 0.8418\n",
      "Epoch 42/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4647 - acc: 0.8383\n",
      "Epoch 43/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4133 - acc: 0.8546\n",
      "Epoch 44/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4190 - acc: 0.8476\n",
      "Epoch 45/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4666 - acc: 0.8228\n",
      "Epoch 46/100\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4245 - acc: 0.8460\n",
      "Epoch 47/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.4599 - acc: 0.8215\n",
      "Epoch 48/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4040 - acc: 0.8617\n",
      "Epoch 49/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4356 - acc: 0.8384\n",
      "Epoch 50/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.3974 - acc: 0.8660\n",
      "Epoch 51/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.4405 - acc: 0.8385\n",
      "Epoch 52/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4097 - acc: 0.8570\n",
      "Epoch 53/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4322 - acc: 0.8405\n",
      "Epoch 54/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.3672 - acc: 0.8759\n",
      "Epoch 55/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4327 - acc: 0.8467\n",
      "Epoch 56/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4336 - acc: 0.8497\n",
      "Epoch 57/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4116 - acc: 0.8548\n",
      "Epoch 58/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.3928 - acc: 0.8666\n",
      "Epoch 59/100\n",
      "47/47 [==============================] - 0s 846us/step - loss: 0.4350 - acc: 0.8450\n",
      "Epoch 60/100\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.4266 - acc: 0.8371\n",
      "Epoch 61/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4563 - acc: 0.8309\n",
      "Epoch 62/100\n",
      "47/47 [==============================] - 0s 694us/step - loss: 0.3850 - acc: 0.8698\n",
      "Epoch 63/100\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.4371 - acc: 0.8401\n",
      "Epoch 64/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.4113 - acc: 0.8566\n",
      "Epoch 65/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4398 - acc: 0.8411\n",
      "Epoch 66/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.4341 - acc: 0.8435\n",
      "Epoch 67/100\n",
      "47/47 [==============================] - 0s 499us/step - loss: 0.4470 - acc: 0.8404\n",
      "Epoch 68/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4266 - acc: 0.8515\n",
      "Epoch 69/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.3329 - acc: 0.8935\n",
      "Epoch 70/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4140 - acc: 0.8561\n",
      "Epoch 71/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4106 - acc: 0.8568\n",
      "Epoch 72/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4112 - acc: 0.8603\n",
      "Epoch 73/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4290 - acc: 0.8423\n",
      "Epoch 74/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.3730 - acc: 0.8786\n",
      "Epoch 75/100\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.4340 - acc: 0.8388\n",
      "Epoch 76/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4271 - acc: 0.8356\n",
      "Epoch 77/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4336 - acc: 0.8306\n",
      "Epoch 78/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 79/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.4462 - acc: 0.8283\n",
      "Epoch 80/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.3904 - acc: 0.8608\n",
      "Epoch 81/100\n",
      "47/47 [==============================] - 0s 715us/step - loss: 0.3945 - acc: 0.8641\n",
      "Epoch 82/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4130 - acc: 0.8526\n",
      "Epoch 83/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.4385 - acc: 0.8411\n",
      "Epoch 84/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4095 - acc: 0.8483\n",
      "Epoch 85/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4195 - acc: 0.8451\n",
      "Epoch 86/100\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.3889 - acc: 0.8646\n",
      "Epoch 87/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.3984 - acc: 0.8549\n",
      "Epoch 88/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4468 - acc: 0.8281\n",
      "Epoch 89/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4660 - acc: 0.8240\n",
      "Epoch 90/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.3881 - acc: 0.8571\n",
      "Epoch 91/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.3860 - acc: 0.8541\n",
      "Epoch 92/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.3664 - acc: 0.8691\n",
      "Epoch 93/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4263 - acc: 0.8435\n",
      "Epoch 94/100\n",
      "47/47 [==============================] - 0s 672us/step - loss: 0.3790 - acc: 0.8743\n",
      "Epoch 95/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.4319 - acc: 0.8363\n",
      "Epoch 96/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.4429 - acc: 0.8420\n",
      "Epoch 97/100\n",
      "47/47 [==============================] - 0s 564us/step - loss: 0.4113 - acc: 0.8474\n",
      "Epoch 98/100\n",
      "47/47 [==============================] - 0s 520us/step - loss: 0.3742 - acc: 0.8673\n",
      "Epoch 99/100\n",
      "47/47 [==============================] - 0s 542us/step - loss: 0.4308 - acc: 0.8400\n",
      "Epoch 100/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4104 - acc: 0.8496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ac3e44370>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(X,y_en,epochs=100,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 499us/step - loss: 0.3993 - acc: 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3993336856365204, 0.8531914949417114]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(X,y_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model4.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964063</td>\n",
       "      <td>0.035937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.421051</td>\n",
       "      <td>0.578949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.387686</td>\n",
       "      <td>0.612314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.397467</td>\n",
       "      <td>0.602533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984820</td>\n",
       "      <td>0.015180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978221</td>\n",
       "      <td>0.021779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978946</td>\n",
       "      <td>0.021054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985210</td>\n",
       "      <td>0.014790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875227</td>\n",
       "      <td>0.124773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.989223</td>\n",
       "      <td>0.010777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1    pred_0    pred_1\n",
       "0    1  0  0.964063  0.035937\n",
       "1    1  0  0.421051  0.578949\n",
       "2    0  1  0.387686  0.612314\n",
       "3    0  1  0.397467  0.602533\n",
       "4    1  0  0.984820  0.015180\n",
       "..  .. ..       ...       ...\n",
       "465  1  0  0.978221  0.021779\n",
       "466  1  0  0.978946  0.021054\n",
       "467  1  0  0.985210  0.014790\n",
       "468  0  1  0.875227  0.124773\n",
       "469  1  0  0.989223  0.010777\n",
       "\n",
       "[470 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_en[\"pred_0\"]=pred[:,0]\n",
    "y_en[\"pred_1\"]=pred[:,1]\n",
    "\n",
    "y_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보스턴 집값 데이터를 이용한 회귀 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train,y_train),(X_test,y_test)=boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13), (404,), (102,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_157 (Dense)            (None, 128)               1792      \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 52)                6708      \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 26)                1378      \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 13)                351       \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 10,243\n",
      "Trainable params: 10,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_boston=Sequential()\n",
    "#입력층\n",
    "model_boston.add(Dense(128,input_dim=13,activation=\"relu\"))\n",
    "#은닉층\n",
    "model_boston.add(Dense(52,activation=\"relu\"))\n",
    "model_boston.add(Dense(26,activation=\"relu\"))\n",
    "model_boston.add(Dense(13,activation=\"relu\"))\n",
    "#출력층\n",
    "model_boston.add(Dense(1,activation=\"linear\"))\n",
    "model_boston.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#컴파일\n",
    "#회귀 모델이므로 mse 또는 mean squared error\n",
    "#metrics는 안써도 됨 -loss만 평가\n",
    "model_boston.compile(loss=\"mse\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ac9a6fbe0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verbose=0:실행과정을 보여주지 않게 설정\n",
    "model_boston.fit(X_train,y_train,epochs=15000,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 499us/step - loss: 0.0369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03685653209686279"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_boston.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMI 데이터셋을 이용한 다중분류 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>45</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>72</td>\n",
       "      <td>fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>61</td>\n",
       "      <td>fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>56</td>\n",
       "      <td>fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192</td>\n",
       "      <td>48</td>\n",
       "      <td>thin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   height  weight   label\n",
       "0     140      45  normal\n",
       "1     145      72     fat\n",
       "2     150      61     fat\n",
       "3     137      56     fat\n",
       "4     192      48    thin"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi=pd.read_csv(\"./data/bmi.csv\")\n",
    "bmi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['normal', 'fat', 'thin'], dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fat       7712\n",
       "thin      6338\n",
       "normal    5950\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=bmi.iloc[:,:2]\n",
    "y=bmi.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.get_dummies(y_train)\n",
    "y_test=pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_180 (Dense)            (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 132,611\n",
      "Trainable params: 132,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#딥러닝을 이용해서 다중분류\n",
    "model_bmi=Sequential()\n",
    "model_bmi.add(Dense(128,input_dim=2,activation=\"relu\"))\n",
    "model_bmi.add(Dense(256,activation=\"relu\"))\n",
    "model_bmi.add(Dense(128,activation=\"relu\"))\n",
    "model_bmi.add(Dense(256,activation=\"relu\"))\n",
    "model_bmi.add(Dense(128,activation=\"relu\"))\n",
    "model_bmi.add(Dense(3,activation=\"softmax\"))\n",
    "model_bmi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bmi.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 2s 875us/step - loss: 0.6793 - acc: 0.7111\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 1s 963us/step - loss: 0.3806 - acc: 0.8238\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 1s 897us/step - loss: 0.1888 - acc: 0.9245 1s - \n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 1s 905us/step - loss: 0.1405 - acc: 0.9394\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 1s 955us/step - loss: 0.1077 - acc: 0.9561\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0941 - acc: 0.9608\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 1s 896us/step - loss: 0.0917 - acc: 0.9613\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 1s 932us/step - loss: 0.0957 - acc: 0.9619\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 1s 940us/step - loss: 0.0880 - acc: 0.9633\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 1s 902us/step - loss: 0.0774 - acc: 0.9671\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 1s 902us/step - loss: 0.0824 - acc: 0.9654\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 1s 872us/step - loss: 0.0895 - acc: 0.9608\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 1s 905us/step - loss: 0.0735 - acc: 0.9704\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 1s 899us/step - loss: 0.0717 - acc: 0.9706\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 1s 854us/step - loss: 0.0771 - acc: 0.9653\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 1s 882us/step - loss: 0.0755 - acc: 0.9689\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 1s 929us/step - loss: 0.0724 - acc: 0.9685\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 1s 969us/step - loss: 0.0692 - acc: 0.9720\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 1s 873us/step - loss: 0.0807 - acc: 0.9673\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 1s 871us/step - loss: 0.0840 - acc: 0.9622\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 1s 861us/step - loss: 0.0751 - acc: 0.9675\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 1s 881us/step - loss: 0.0670 - acc: 0.9737\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 1s 876us/step - loss: 0.0785 - acc: 0.9684\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 1s 854us/step - loss: 0.0685 - acc: 0.9738\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 1s 898us/step - loss: 0.0699 - acc: 0.9691\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 1s 878us/step - loss: 0.0681 - acc: 0.9719\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 1s 918us/step - loss: 0.0745 - acc: 0.9700\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 1s 866us/step - loss: 0.0641 - acc: 0.9716\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 1s 980us/step - loss: 0.0668 - acc: 0.9741\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 1s 951us/step - loss: 0.0772 - acc: 0.9665\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 1s 870us/step - loss: 0.0693 - acc: 0.9706\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 1s 842us/step - loss: 0.0665 - acc: 0.9712\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 1s 867us/step - loss: 0.0619 - acc: 0.9747\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 1s 890us/step - loss: 0.0627 - acc: 0.9731\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 1s 915us/step - loss: 0.0568 - acc: 0.9759\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 1s 896us/step - loss: 0.0695 - acc: 0.9703\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 1s 871us/step - loss: 0.0669 - acc: 0.9698\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 1s 896us/step - loss: 0.0643 - acc: 0.9726\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 1s 857us/step - loss: 0.0651 - acc: 0.9739\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 1s 932us/step - loss: 0.0584 - acc: 0.9754\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 1s 994us/step - loss: 0.0532 - acc: 0.9767\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 1s 933us/step - loss: 0.0638 - acc: 0.9738\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 1s 889us/step - loss: 0.0609 - acc: 0.9752\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 1s 855us/step - loss: 0.0578 - acc: 0.9758\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 1s 869us/step - loss: 0.0629 - acc: 0.9720\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 1s 883us/step - loss: 0.0568 - acc: 0.9747\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 1s 899us/step - loss: 0.0650 - acc: 0.9742\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 1s 883us/step - loss: 0.0561 - acc: 0.9771\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 1s 881us/step - loss: 0.0536 - acc: 0.9778\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 1s 911us/step - loss: 0.0604 - acc: 0.9712\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 1s 896us/step - loss: 0.0649 - acc: 0.9728\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 1s 888us/step - loss: 0.0585 - acc: 0.9750\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0579 - acc: 0.9768\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 1s 937us/step - loss: 0.0621 - acc: 0.9737\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 1s 910us/step - loss: 0.0810 - acc: 0.9694\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 1s 875us/step - loss: 0.0552 - acc: 0.9768\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 1s 854us/step - loss: 0.0563 - acc: 0.9755\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 1s 867us/step - loss: 0.0674 - acc: 0.9703\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 1s 866us/step - loss: 0.0575 - acc: 0.9762\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 1s 863us/step - loss: 0.0496 - acc: 0.9788\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 1s 868us/step - loss: 0.0570 - acc: 0.9765\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 1s 911us/step - loss: 0.0561 - acc: 0.9761\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 1s 894us/step - loss: 0.0654 - acc: 0.9721\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 1s 918us/step - loss: 0.0475 - acc: 0.9795\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0568 - acc: 0.9760\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 1s 888us/step - loss: 0.0541 - acc: 0.9766\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 1s 885us/step - loss: 0.0578 - acc: 0.9752\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 1s 867us/step - loss: 0.0572 - acc: 0.9761\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 1s 888us/step - loss: 0.0546 - acc: 0.9760\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 1s 883us/step - loss: 0.0598 - acc: 0.9743\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 1s 858us/step - loss: 0.0579 - acc: 0.9754\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 1s 864us/step - loss: 0.0592 - acc: 0.9745\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 1s 851us/step - loss: 0.0509 - acc: 0.9776\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 1s 885us/step - loss: 0.0740 - acc: 0.9721\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 1s 905us/step - loss: 0.0501 - acc: 0.9786\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 1s 874us/step - loss: 0.0595 - acc: 0.9736\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0553 - acc: 0.9747\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 1s 900us/step - loss: 0.0558 - acc: 0.9770\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 1s 902us/step - loss: 0.0513 - acc: 0.9791\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 1s 891us/step - loss: 0.0488 - acc: 0.9795\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 1s 892us/step - loss: 0.0623 - acc: 0.9710\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 1s 892us/step - loss: 0.0527 - acc: 0.9769\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 1s 884us/step - loss: 0.0569 - acc: 0.9782\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 1s 941us/step - loss: 0.0609 - acc: 0.9738\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 1s 875us/step - loss: 0.0457 - acc: 0.9821\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 1s 856us/step - loss: 0.0557 - acc: 0.9747\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 1s 886us/step - loss: 0.0416 - acc: 0.9817\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 1s 890us/step - loss: 0.0469 - acc: 0.9812\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0615 - acc: 0.9749\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 1s 882us/step - loss: 0.0533 - acc: 0.9773\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 1s 887us/step - loss: 0.0543 - acc: 0.9779\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 1s 912us/step - loss: 0.0449 - acc: 0.9807\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 1s 877us/step - loss: 0.0534 - acc: 0.9758\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 1s 902us/step - loss: 0.0468 - acc: 0.9806\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 1s 892us/step - loss: 0.0512 - acc: 0.9796\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 1s 891us/step - loss: 0.0466 - acc: 0.9794\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 1s 901us/step - loss: 0.0601 - acc: 0.9773\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 1s 885us/step - loss: 0.0521 - acc: 0.9798\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 1s 925us/step - loss: 0.0639 - acc: 0.9745\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 1s 964us/step - loss: 0.0540 - acc: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ac9cca160>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bmi.fit(X_train,y_train,epochs=100,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 620us/step - loss: 0.0565 - acc: 0.9712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.056547705084085464, 0.9711999893188477]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bmi.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
